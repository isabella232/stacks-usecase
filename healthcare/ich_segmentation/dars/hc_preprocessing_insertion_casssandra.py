# -*- coding: utf-8 -*-
"""segmentation_physionet

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RDhdupGHBOkZaSDy0PagzWoWaK4IW4-w
"""

import os
import sys
import io
import pathlib
import pyspark
from pathlib import Path

import nibabel as nib
import numpy as np
import pandas as pd
from PIL import Image

from cassandra.cluster import Cluster
import uuid, argparse,ssl

from pyspark import SparkContext
sc = SparkContext(appName = "dars-hc")

class ICHDataPreprocess:
    """preprocess ICH data as detailed in the paper.

    src: https://alpha.physionet.org/content/ct-ich/1.0.0/
    """
    db_host = []

    def _resize(self, img: np.ndarray, size=151) -> np.ndarray:
        image = Image.fromarray(img.astype(np.int8), mode="L")
        return image.resize((size, size), resample=Image.BICUBIC)

    def _window_ct(self, ct_scan, w_level=40, w_width=120):
        w_min = w_level - w_width / 2
        w_max = w_level + w_width / 2
        num_slices = ct_scan.shape[2]
        for s in range(num_slices):
            slice_s = ct_scan[:, :, s]
            slice_s = (slice_s - w_min) * (255 / (w_max - w_min))
            slice_s[slice_s < 0] = 0
            slice_s[slice_s > 255] = 255
            ct_scan[:, :, s] = slice_s
        return ct_scan

    def preprocess(self,host):
        self.db_host = host

        # raw data path , change to where the raw data is downloaded.
        base_path = "/mnt/data/"
        dir_name = "physionet.org/files/ct-ich/1.3.1"
        base_data_dir = os.path.join(base_path, dir_name)
        enable = 0

        cluster = Cluster(self.db_host)
        session = cluster.connect('healthcare_keyspace_testing_dars')
        blob_insertion = session.prepare("INSERT INTO processed_data(id,date,is_label,patient_number,slice_number,intraventricular,intraparenchymal,subarachnoid,epidural,subdural,no_hemorrhage,fracture_yes_no,image_filename,image_blob) VALUES ( UUID() , TOUNIXTIMESTAMP(now()), ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ? )")

        if enable < 1:
            enable=1
            num_subjects = 82
            new_size = 512
            window_specs = [40, 120]  # brain window
            hemorrhage_diagnosis_df = pd.read_csv(
                Path(base_data_dir, "hemorrhage_diagnosis_raw_ct.csv")
            )
            hemorrhage_diagnosis_array = hemorrhage_diagnosis_df._get_values
            hemorrhage_diagnosis_df.set_index("PatientNumber", inplace=True)
            counter = 0

            for n in range(0 + 49, num_subjects + 49):

                if n > 58 and n < 66:  # no raw data were available for these subjects
                    next
                else:
                    ct_dir_subj = pathlib.Path(
                        base_data_dir, "ct_scans", "{0:0=3d}.nii".format(n)
                    )
                    ct_scan_nifti = nib.load(str(ct_dir_subj))
                    ct_scan = ct_scan_nifti.get_fdata()
                    ct_scan = self._window_ct(ct_scan, window_specs[0], window_specs[1])

                    masks_dir_subj = pathlib.Path(
                        base_data_dir, "masks", "{0:0=3d}.nii".format(n)
                    )
                    masks_nifti = nib.load(str(masks_dir_subj))
                    masks = masks_nifti.get_fdata()

                    idx = hemorrhage_diagnosis_array[:, 0] == n
                    patient_data_fields_df = hemorrhage_diagnosis_df.loc[[n]]
                    patient_data_fields_df.set_index("SliceNumber", inplace=True)

                    sliceNos = hemorrhage_diagnosis_array[idx, 1]
                    if sliceNos.size != ct_scan.shape[2]:
                        print(
                            "Warning: the number of annotated slices does not equal the number of slices in NIFTI file!"
                        )

                    for sliceI in range(0, sliceNos.size):
                        slice_idx = sliceI + 1
                        slice_data_fields_df = patient_data_fields_df.loc[[slice_idx]]

                        x = ct_scan[:, :, sliceI]
                        x = self._resize(x, new_size)
                        id_img = str(counter) + ".png"

                        buf = io.BytesIO()
                        x.save(buf, format='PNG')
                        imageBlob = buf.getvalue()
                        is_label=0
                        session.execute(blob_insertion, [is_label,n,slice_idx,slice_data_fields_df.iat[0,0],slice_data_fields_df.iat[0,1],slice_data_fields_df.iat[0,2],slice_data_fields_df.iat[0,3],slice_data_fields_df.iat[0,4],slice_data_fields_df.iat[0,5],slice_data_fields_df.iat[0,6],id_img,imageBlob])

                        is_label=1
                        x = self._resize(masks[:, :, sliceI], new_size)
                        buf_label = io.BytesIO()
                        x.save(buf_label, format='PNG')
                        labelBlob = buf_label.getvalue()
                        session.execute(blob_insertion, [is_label,n,slice_idx,slice_data_fields_df.iat[0,0],slice_data_fields_df.iat[0,1],slice_data_fields_df.iat[0,2],slice_data_fields_df.iat[0,3],slice_data_fields_df.iat[0,4],slice_data_fields_df.iat[0,5],slice_data_fields_df.iat[0,6],id_img,labelBlob])

                        counter = counter + 1

if __name__ == "__main__":
    # Cassandra setup, change to the corresponding Cassandra hostname or IP.
    hostDB  = ['127.0.0.1']

    data_preprocess = ICHDataPreprocess()
    data_preprocess.preprocess(hostDB)
